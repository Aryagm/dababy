{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac25115",
   "metadata": {},
   "source": [
    "# Baby Cry Classification Training Notebook\n",
    "\n",
    "This notebook provides a complete training pipeline for classifying baby cries into 8 different categories using deep learning techniques.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset includes 1126 audio files divided into 8 directories, each representing a specific reason for crying:\n",
    "\n",
    "- **Belly pain**: 133 files\n",
    "- **Burping**: 124 files\n",
    "- **Cold or hot**: 130 files\n",
    "- **Discomfort**: 142 files\n",
    "- **Hungry**: 397 files\n",
    "- **Lonely**: 25 files\n",
    "- **Scared**: 33 files\n",
    "- **Tired**: 142 files\n",
    "\n",
    "**Data Characteristics:**\n",
    "\n",
    "- Format: Audio files (1039 .wav, 8 .ogg, 7 other formats)\n",
    "- Language: Not applicable (infant cries only)\n",
    "- Total files: 1126\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- TensorFlow 2.x\n",
    "- librosa for audio processing\n",
    "- numpy, pandas for data manipulation\n",
    "- matplotlib, seaborn for visualization\n",
    "- scikit-learn for metrics and preprocessing\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "1. **Data Loading & Exploration**\n",
    "2. **Audio Preprocessing & Feature Extraction**\n",
    "3. **Data Augmentation**\n",
    "4. **Model Architecture Design**\n",
    "5. **Training with Validation**\n",
    "6. **Model Evaluation**\n",
    "7. **Model Saving & Inference**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Sklearn for preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU will be used for training\")\n",
    "else:\n",
    "    print(\"CPU will be used for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6695a4",
   "metadata": {},
   "source": [
    "## 1. Dataset Configuration and Loading\n",
    "\n",
    "First, let's set up the dataset path and load all audio files with their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_PATH = \"data/baby_cries\"  # Update this path to your dataset location\n",
    "SAMPLE_RATE = 22050  # Standard sample rate for audio processing\n",
    "DURATION = 3.0  # Duration in seconds to standardize audio length\n",
    "N_SAMPLES = int(SAMPLE_RATE * DURATION)\n",
    "\n",
    "# Define class labels\n",
    "CLASS_LABELS = [\n",
    "    'belly_pain',\n",
    "    'burping', \n",
    "    'cold_hot',\n",
    "    'discomfort',\n",
    "    'hungry',\n",
    "    'lonely',\n",
    "    'scared',\n",
    "    'tired'\n",
    "]\n",
    "\n",
    "# Expected file counts per class (from dataset description)\n",
    "EXPECTED_COUNTS = {\n",
    "    'belly_pain': 133,\n",
    "    'burping': 124,\n",
    "    'cold_hot': 130,\n",
    "    'discomfort': 142,\n",
    "    'hungry': 397,\n",
    "    'lonely': 25,\n",
    "    'scared': 33,\n",
    "    'tired': 142\n",
    "}\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Sample rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"Audio duration: {DURATION} seconds\")\n",
    "print(f\"Number of samples per audio: {N_SAMPLES}\")\n",
    "print(f\"Number of classes: {len(CLASS_LABELS)}\")\n",
    "print(f\"Total expected files: {sum(EXPECTED_COUNTS.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60057c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(dataset_path):\n",
    "    \"\"\"\n",
    "    Load all audio files from the dataset directory.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (file_paths, labels, class_counts)\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    print(\"Loading audio files...\")\n",
    "    \n",
    "    for class_label in CLASS_LABELS:\n",
    "        class_dir = os.path.join(dataset_path, class_label)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Warning: Directory {class_dir} not found!\")\n",
    "            continue\n",
    "            \n",
    "        # Get all audio files in the class directory\n",
    "        audio_extensions = ['*.wav', '*.ogg', '*.mp3', '*.flac', '*.m4a']\n",
    "        class_files = []\n",
    "        \n",
    "        for ext in audio_extensions:\n",
    "            class_files.extend(glob.glob(os.path.join(class_dir, ext)))\n",
    "        \n",
    "        class_counts[class_label] = len(class_files)\n",
    "        print(f\"{class_label}: {len(class_files)} files\")\n",
    "        \n",
    "        # Add to main lists\n",
    "        file_paths.extend(class_files)\n",
    "        labels.extend([class_label] * len(class_files))\n",
    "    \n",
    "    print(f\"\\nTotal files loaded: {len(file_paths)}\")\n",
    "    return file_paths, labels, class_counts\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    file_paths, labels, class_counts = load_audio_files(DATASET_PATH)\n",
    "    \n",
    "    # Create a DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'file_path': file_paths,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Classes: {df['label'].unique()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating sample data structure for demonstration...\")\n",
    "    \n",
    "    # Create sample data for demonstration if dataset not found\n",
    "    df = pd.DataFrame({\n",
    "        'file_path': [f\"sample_{i}.wav\" for i in range(100)],\n",
    "        'label': np.random.choice(CLASS_LABELS, 100)\n",
    "    })\n",
    "    class_counts = df['label'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration and visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Class distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "class_counts_series = df['label'].value_counts()\n",
    "plt.bar(class_counts_series.index, class_counts_series.values, color='skyblue')\n",
    "plt.title('Distribution of Baby Cry Classes')\n",
    "plt.xlabel('Cry Type')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(class_counts_series.values):\n",
    "    plt.text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 2. Expected vs Actual counts comparison (if actual data is available)\n",
    "plt.subplot(2, 2, 2)\n",
    "expected_df = pd.DataFrame(list(EXPECTED_COUNTS.items()), columns=['label', 'expected'])\n",
    "actual_df = pd.DataFrame(list(class_counts_series.items()), columns=['label', 'actual'])\n",
    "comparison_df = pd.merge(expected_df, actual_df, on='label', how='outer').fillna(0)\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, comparison_df['expected'], width, label='Expected', alpha=0.7)\n",
    "plt.bar(x + width/2, comparison_df['actual'], width, label='Actual', alpha=0.7)\n",
    "plt.title('Expected vs Actual File Counts')\n",
    "plt.xlabel('Cry Type')\n",
    "plt.ylabel('Number of Files')\n",
    "plt.xticks(x, comparison_df['label'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Class imbalance visualization\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.pie(class_counts_series.values, labels=class_counts_series.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "\n",
    "# 4. Class statistics\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "Dataset Statistics:\n",
    "• Total files: {len(df)}\n",
    "• Number of classes: {len(class_counts_series)}\n",
    "• Largest class: {class_counts_series.index[0]} ({class_counts_series.iloc[0]} files)\n",
    "• Smallest class: {class_counts_series.index[-1]} ({class_counts_series.iloc[-1]} files)\n",
    "• Imbalance ratio: {class_counts_series.iloc[0] / class_counts_series.iloc[-1]:.1f}:1\n",
    "• Mean files per class: {class_counts_series.mean():.1f}\n",
    "• Std files per class: {class_counts_series.std():.1f}\n",
    "\"\"\"\n",
    "plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Class Statistics:\")\n",
    "print(f\"{'Class':<15} {'Count':<8} {'Percentage':<12}\")\n",
    "print(\"-\" * 35)\n",
    "for label, count in class_counts_series.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{label:<15} {count:<8} {percentage:<12.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ea98",
   "metadata": {},
   "source": [
    "## 2. Audio Preprocessing and Feature Extraction\n",
    "\n",
    "Now let's implement functions to load, preprocess audio files and extract features for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, sample_rate=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single audio file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file\n",
    "        sample_rate (int): Target sample rate\n",
    "        duration (float): Target duration in seconds\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Preprocessed audio signal\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(file_path, sr=sample_rate, duration=duration)\n",
    "        \n",
    "        # Pad or truncate to fixed length\n",
    "        target_length = int(sample_rate * duration)\n",
    "        if len(audio) < target_length:\n",
    "            # Pad with zeros if too short\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "        else:\n",
    "            # Truncate if too long\n",
    "            audio = audio[:target_length]\n",
    "        \n",
    "        # Normalize audio\n",
    "        audio = audio / np.max(np.abs(audio) + 1e-6)\n",
    "        \n",
    "        return audio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        # Return zeros if file can't be loaded\n",
    "        return np.zeros(int(sample_rate * duration))\n",
    "\n",
    "def extract_mfcc_features(audio, sample_rate=SAMPLE_RATE, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from audio signal.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.array): Audio signal\n",
    "        sample_rate (int): Sample rate\n",
    "        n_mfcc (int): Number of MFCC coefficients\n",
    "        n_fft (int): FFT window size\n",
    "        hop_length (int): Hop length for STFT\n",
    "        \n",
    "    Returns:\n",
    "        np.array: MFCC features\n",
    "    \"\"\"\n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio, \n",
    "        sr=sample_rate, \n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    # Add delta and delta-delta features\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate([mfcc, mfcc_delta, mfcc_delta2], axis=0)\n",
    "    \n",
    "    return features.T  # Transpose to (time, features)\n",
    "\n",
    "def extract_spectrogram_features(audio, sample_rate=SAMPLE_RATE, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract mel-spectrogram features from audio signal.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.array): Audio signal\n",
    "        sample_rate (int): Sample rate\n",
    "        n_fft (int): FFT window size\n",
    "        hop_length (int): Hop length for STFT\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Mel-spectrogram features\n",
    "    \"\"\"\n",
    "    # Extract mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=128\n",
    "    )\n",
    "    \n",
    "    # Convert to log scale\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    return log_mel_spec.T  # Transpose to (time, frequency)\n",
    "\n",
    "def augment_audio(audio, sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Apply audio augmentation techniques.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.array): Original audio signal\n",
    "        sample_rate (int): Sample rate\n",
    "        \n",
    "    Returns:\n",
    "        list: List of augmented audio signals\n",
    "    \"\"\"\n",
    "    augmented = [audio]  # Include original\n",
    "    \n",
    "    # Time shifting\n",
    "    shift_max = int(0.2 * sample_rate)  # 0.2 seconds\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    augmented.append(np.roll(audio, shift))\n",
    "    \n",
    "    # Speed change\n",
    "    speed_factor = np.random.uniform(0.8, 1.2)\n",
    "    stretched = librosa.effects.time_stretch(audio, rate=speed_factor)\n",
    "    if len(stretched) >= len(audio):\n",
    "        augmented.append(stretched[:len(audio)])\n",
    "    else:\n",
    "        padded = np.pad(stretched, (0, len(audio) - len(stretched)), mode='constant')\n",
    "        augmented.append(padded)\n",
    "    \n",
    "    # Pitch shifting\n",
    "    pitch_shift = np.random.randint(-2, 3)\n",
    "    if pitch_shift != 0:\n",
    "        pitched = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=pitch_shift)\n",
    "        augmented.append(pitched)\n",
    "    \n",
    "    # Add noise\n",
    "    noise_factor = 0.005\n",
    "    noise = np.random.normal(0, noise_factor, len(audio))\n",
    "    noisy = audio + noise\n",
    "    augmented.append(noisy)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "print(\"Audio preprocessing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature extraction with a sample audio file\n",
    "def visualize_audio_features(file_path, label):\n",
    "    \"\"\"\n",
    "    Visualize audio waveform and extracted features.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio = load_and_preprocess_audio(file_path)\n",
    "    \n",
    "    # Extract features\n",
    "    mfcc_features = extract_mfcc_features(audio)\n",
    "    spec_features = extract_spectrogram_features(audio)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Waveform\n",
    "    time = np.linspace(0, DURATION, len(audio))\n",
    "    axes[0, 0].plot(time, audio)\n",
    "    axes[0, 0].set_title(f'Waveform - {label}')\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # 2. MFCC features\n",
    "    axes[0, 1].imshow(mfcc_features.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, 1].set_title('MFCC Features')\n",
    "    axes[0, 1].set_xlabel('Time Frame')\n",
    "    axes[0, 1].set_ylabel('MFCC Coefficient')\n",
    "    \n",
    "    # 3. Mel-spectrogram\n",
    "    axes[1, 0].imshow(spec_features.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, 0].set_title('Mel-Spectrogram')\n",
    "    axes[1, 0].set_xlabel('Time Frame')\n",
    "    axes[1, 0].set_ylabel('Mel Frequency')\n",
    "    \n",
    "    # 4. Feature statistics\n",
    "    axes[1, 1].axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "    Audio Statistics:\n",
    "    • Duration: {DURATION} seconds\n",
    "    • Sample Rate: {SAMPLE_RATE} Hz\n",
    "    • Audio Shape: {audio.shape}\n",
    "    • MFCC Shape: {mfcc_features.shape}\n",
    "    • Spectrogram Shape: {spec_features.shape}\n",
    "    • Audio Min/Max: {audio.min():.3f} / {audio.max():.3f}\n",
    "    • Audio Mean/Std: {audio.mean():.3f} / {audio.std():.3f}\n",
    "    \"\"\"\n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return audio, mfcc_features, spec_features\n",
    "\n",
    "# Visualize features for the first available file (if dataset exists)\n",
    "if len(df) > 0 and os.path.exists(df.iloc[0]['file_path']):\n",
    "    print(\"Visualizing features for first audio file...\")\n",
    "    sample_file = df.iloc[0]['file_path']\n",
    "    sample_label = df.iloc[0]['label']\n",
    "    visualize_audio_features(sample_file, sample_label)\n",
    "else:\n",
    "    print(\"Dataset not found - skipping visualization\")\n",
    "    print(\"Feature extraction functions are ready to use when dataset is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afdcd4",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Design\n",
    "\n",
    "Let's create a deep learning model suitable for audio classification using both CNN and RNN components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c73aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d_model(input_shape, num_classes=8):\n",
    "    \"\"\"\n",
    "    Create a 1D CNN model for audio classification using MFCC features.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input features (time_steps, features)\n",
    "        num_classes (int): Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First Conv1D block\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Conv1D block\n",
    "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Conv1D block\n",
    "        layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Global pooling and dense layers\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_conv2d_model(input_shape, num_classes=8):\n",
    "    \"\"\"\n",
    "    Create a 2D CNN model for audio classification using spectrogram features.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input features (height, width, channels)\n",
    "        num_classes (int): Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First Conv2D block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Conv2D block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Conv2D block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fourth Conv2D block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes=8):\n",
    "    \"\"\"\n",
    "    Create an LSTM model for sequential audio classification.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input features (time_steps, features)\n",
    "        num_classes (int): Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # LSTM layers\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_hybrid_model(input_shape, num_classes=8):\n",
    "    \"\"\"\n",
    "    Create a hybrid CNN+LSTM model for audio classification.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input features (time_steps, features)\n",
    "        num_classes (int): Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv1D layers for feature extraction\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # LSTM layers for temporal modeling\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2),\n",
    "        layers.LSTM(64, return_sequences=False, dropout=0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model architectures defined successfully!\")\n",
    "print(\"Available models:\")\n",
    "print(\"1. Conv1D model - for MFCC features\")\n",
    "print(\"2. Conv2D model - for spectrogram features\") \n",
    "print(\"3. LSTM model - for sequential features\")\n",
    "print(\"4. Hybrid CNN+LSTM model - combines both approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25136044",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Model Training\n",
    "\n",
    "Now let's prepare the data and train our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2adc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, use_augmentation=True, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with file paths and labels\n",
    "        use_augmentation (bool): Whether to apply data augmentation\n",
    "        test_size (float): Proportion of test set\n",
    "        val_size (float): Proportion of validation set (from training data)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Prepared datasets and label encoder\n",
    "    \"\"\"\n",
    "    print(\"Preparing dataset...\")\n",
    "    \n",
    "    # Initialize label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(df['label'])\n",
    "    \n",
    "    # Split data into train and test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df['file_path'].values, encoded_labels, \n",
    "        test_size=test_size, stratify=encoded_labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=val_size, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\") \n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Apply data augmentation to training set\n",
    "    if use_augmentation:\n",
    "        print(\"Applying data augmentation...\")\n",
    "        augmented_paths = []\n",
    "        augmented_labels = []\n",
    "        \n",
    "        for path, label in zip(X_train, y_train):\n",
    "            augmented_paths.append(path)  # Original\n",
    "            augmented_labels.append(label)\n",
    "            \n",
    "            # Add augmented versions (limit to balance dataset)\n",
    "            if label_encoder.inverse_transform([label])[0] in ['lonely', 'scared']:  # Minority classes\n",
    "                # More augmentation for minority classes\n",
    "                for _ in range(3):\n",
    "                    augmented_paths.append(path)\n",
    "                    augmented_labels.append(label)\n",
    "            elif label_encoder.inverse_transform([label])[0] != 'hungry':  # Not majority class\n",
    "                # Some augmentation for other classes\n",
    "                augmented_paths.append(path)\n",
    "                augmented_labels.append(label)\n",
    "        \n",
    "        X_train = np.array(augmented_paths)\n",
    "        y_train = np.array(augmented_labels)\n",
    "        print(f\"Augmented train set: {len(X_train)} samples\")\n",
    "    \n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), label_encoder\n",
    "\n",
    "def load_features_batch(file_paths, labels, feature_type='mfcc', apply_augmentation=False):\n",
    "    \"\"\"\n",
    "    Load and extract features for a batch of files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (array): Array of file paths\n",
    "        labels (array): Array of labels\n",
    "        feature_type (str): Type of features to extract ('mfcc' or 'spectrogram')\n",
    "        apply_augmentation (bool): Whether to apply augmentation during loading\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (features, labels)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    processed_labels = []\n",
    "    \n",
    "    print(f\"Loading {len(file_paths)} files with {feature_type} features...\")\n",
    "    \n",
    "    for i, (file_path, label) in enumerate(zip(file_paths, labels)):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(file_paths)} files\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            if os.path.exists(file_path):\n",
    "                audio = load_and_preprocess_audio(file_path)\n",
    "            else:\n",
    "                # Create dummy data if file doesn't exist (for demo)\n",
    "                audio = np.random.randn(N_SAMPLES) * 0.1\n",
    "            \n",
    "            # Apply augmentation if requested\n",
    "            audio_variants = [audio]\n",
    "            if apply_augmentation:\n",
    "                audio_variants = augment_audio(audio)\n",
    "            \n",
    "            # Extract features for each variant\n",
    "            for audio_variant in audio_variants:\n",
    "                if feature_type == 'mfcc':\n",
    "                    feature = extract_mfcc_features(audio_variant)\n",
    "                else:  # spectrogram\n",
    "                    feature = extract_spectrogram_features(audio_variant)\n",
    "                \n",
    "                features.append(feature)\n",
    "                processed_labels.append(label)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(features), np.array(processed_labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "if len(df) > 0:\n",
    "    print(\"Preparing real dataset...\")\n",
    "    datasets, label_encoder = prepare_dataset(df, use_augmentation=True)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = datasets\n",
    "else:\n",
    "    print(\"Creating demo dataset...\")\n",
    "    # Create demo data\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(CLASS_LABELS)\n",
    "    \n",
    "    # Generate dummy file paths and labels\n",
    "    n_samples = 100\n",
    "    demo_paths = [f\"demo_file_{i}.wav\" for i in range(n_samples)]\n",
    "    demo_labels = np.random.choice(range(len(CLASS_LABELS)), n_samples)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        demo_paths, demo_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Label classes: {label_encoder.classes_}\")\n",
    "print(f\"Final dataset sizes - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Train a model with proper callbacks and monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model to train\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        model_name (str): Name for saving model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_model, history)\n",
    "    \"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\\\n{model_name} Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'models/best_{model_name}.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plot training history.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_title(f'{model_name} - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_title(f'{model_name} - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(\"Training configuration set!\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(\"Ready to train models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and compare performance\n",
    "\n",
    "# Model 1: Conv1D with MFCC features\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING CONV1D MODEL WITH MFCC FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Load MFCC features for training\n",
    "    X_train_mfcc, y_train_aug = load_features_batch(\n",
    "        X_train, y_train, feature_type='mfcc', apply_augmentation=True\n",
    "    )\n",
    "    X_val_mfcc, _ = load_features_batch(\n",
    "        X_val, y_val, feature_type='mfcc', apply_augmentation=False\n",
    "    )\n",
    "    \n",
    "    # Create and train Conv1D model\n",
    "    conv1d_model = create_conv1d_model(X_train_mfcc.shape[1:], num_classes=len(CLASS_LABELS))\n",
    "    conv1d_model, conv1d_history = train_model(\n",
    "        conv1d_model, X_train_mfcc, y_train_aug, X_val_mfcc, y_val, \"Conv1D_MFCC\"\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(conv1d_history, \"Conv1D MFCC Model\")\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not available - creating demo model...\")\n",
    "    # Create demo model with sample shapes\n",
    "    sample_mfcc_shape = (130, 39)  # Typical MFCC shape\n",
    "    conv1d_model = create_conv1d_model(sample_mfcc_shape, num_classes=len(CLASS_LABELS))\n",
    "    print(\"Conv1D model created successfully!\")\n",
    "    conv1d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383017fa",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Testing\n",
    "\n",
    "Let's evaluate our trained models and compare their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, label_encoder, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained keras model\n",
    "        X_test, y_test: Test data\n",
    "        label_encoder: Label encoder for class names\n",
    "        model_name (str): Name of the model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\\\n{model_name} Results:\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred, \n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=label_encoder.classes_,\n",
    "        yticklabels=label_encoder.classes_\n",
    "    )\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    print(\"\\\\nPer-class Accuracy:\")\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        print(f\"{class_name:<15}: {class_accuracy[i]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_accuracy': class_accuracy\n",
    "    }\n",
    "\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"\n",
    "    Compare multiple model results.\n",
    "    \n",
    "    Args:\n",
    "        results_dict (dict): Dictionary of model results\n",
    "    \"\"\"\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': results['accuracy'],\n",
    "            'Best Class': label_encoder.classes_[np.argmax(results['class_accuracy'])],\n",
    "            'Worst Class': label_encoder.classes_[np.argmin(results['class_accuracy'])],\n",
    "            'Best Class Acc': np.max(results['class_accuracy']),\n",
    "            'Worst Class Acc': np.min(results['class_accuracy'])\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(comparison_df['Model'], comparison_df['Test Accuracy'], color='skyblue')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for i, v in enumerate(comparison_df['Test Accuracy']):\n",
    "        plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    models = comparison_df['Model']\n",
    "    best_accs = comparison_df['Best Class Acc']\n",
    "    worst_accs = comparison_df['Worst Class Acc']\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, best_accs, width, label='Best Class', alpha=0.7)\n",
    "    plt.bar(x + width/2, worst_accs, width, label='Worst Class', alpha=0.7)\n",
    "    plt.title('Per-Class Performance Range')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the trained model(s)\n",
    "results_dict = {}\n",
    "\n",
    "if len(df) > 0 and 'conv1d_model' in locals():\n",
    "    print(\"Loading test features...\")\n",
    "    X_test_mfcc, _ = load_features_batch(\n",
    "        X_test, y_test, feature_type='mfcc', apply_augmentation=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate Conv1D model\n",
    "    conv1d_results = evaluate_model(\n",
    "        conv1d_model, X_test_mfcc, y_test, label_encoder, \"Conv1D MFCC\"\n",
    "    )\n",
    "    results_dict['Conv1D MFCC'] = conv1d_results\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not available or model not trained - skipping evaluation\")\n",
    "    print(\"Evaluation functions are ready to use when data and models are available\")\n",
    "\n",
    "# Compare models if we have results\n",
    "if results_dict:\n",
    "    compare_models(results_dict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
